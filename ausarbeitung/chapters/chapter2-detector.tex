%!TEX root = ../Thesis.tex

\chapter{Detecting the board}
	\label{detector}
	The lines of a Go board catch one's eye first and foremost when looking at it. However, relying solely on lines for the detection of the grid will probably fail in situations when too many tokens occlude them. Therefore, our algorithm uses them only to find intersections and tries to fill in the gaps by adding information about pieces. The reason behind this is that what is actually interesting are not the lines, but their intersections and a piece will always lie reasonably close to one. The only user interaction happens in the beginning of a recording session. The user has to place the camera so that the center of the board aligns with the center of the camera.

	The implementation\footnote{The source code is GPLv3 licensed and can be freely accessed at https://github.com/t-animal/GoBoardReader} is based on the OpenCV framework (version 2.4.10). If not noted otherwise all detections have been performed on an x86\_64 architecture. As described in chapter 3 this does not give different results than the execution on mobile devices (i.e. ARM architecture), for example because of differences in floating point calculations.

	In short we do the following steps:
	\begin{enumerate}
		\item roughly pre-segment the board by analyzing connected components around the center of the thresholded input image
		\item detect horizontal and vertical lines and intersect them
		\item detect pieces on the board and consider their centers to be intersections, too
		\item remove duplicates
		\item select a few intersections around the center of the image
		\item build a submodel of the board by estimating where each selected point lies on the grid
		\item calculate their position in space using RANSAC and apply the resulting transformation matrix to a complete model of the board
		\item refine the locations by shifting extrapolated intersections to measured ones and redo the calculations
		\item extract piece color information by the amount of black pixels in a window around each intersection in a thresholded version of the input image
	\end{enumerate}

	\section{Preprocessing}
	\label{detector-preprocessing}
	Mobile devices do not have the same computing power as desktop hardware. Therefore our first step is to reduce the image size without losing relevant information. To do so, we threshold the grayscale input image using a mean adaptive threshold with a low constant value \emph{C} and a window size of what we expected to be roughly the width of one square on the board (approximately 45px, as measured in one of our sample images).

	Assuming that at a least part of the board is in the center of the image we can segment it from the background with high confidence by a simple connected-component analysis with a black rectangle in the image center. On our test case this fails only in one situation in which the board lies in grass in the evening. Shadows connect the board with the background and the whole image is segmented. On a flat surface this is not a problem and we found no image where we crop off too much and thus make the board undetectable.

	This step does not just improve speed but also detection performance because interfering background information like patterned wood table tops can be cut off.

	\section{Detecting visible intersections}
	\subsection{By intersecting lines}
	Having cropped the image to the board we use line detection to find the visible grid lines. Our goal is to detect the intersections and the most obvious approach to find them is to detect the visible line segments, continue them into infinity and intersect the results. This way all visible intersections should be detected plus some occluded intersections where enough of a line is visible, for example, because only one piece lies on it. When intersecting those lines that are split into several line segments by a piece, duplicates arise. We filter those in the intersection step rather than trying to find line segments that belong together beforehand.

	We evaluated two different well known line detection methods.
	\subsubsection{Using Hough lines detection}
	\label{detector-visible-hough}
	Our first approach was to use \emph{Probabilistic Hough Line Transform} after applying a \emph{Canny edge detector}. We furthermore found, that blurring the image with a light \emph{Gaussian} improves detection quality and speed.

	For each line the absolute pitch to the x-axis and the y-axis of the image is calculated and compared to a specified threshold. Initially, we set this pretty high to eliminate as many false lines as possible from the image. Later it turned out that the image segmentation in the first step allows us to lower the value to one. That means each line is either classified as horizontal or vertical and only perfect diagonal lines are discarded.

	\subsubsection{Using LSD}
	\label{detector-visible-lsd}
	%TODO update with  updated algorithm
	Next, we tried to apply von Gioi's Line Segment Detector \cite{von2012lsd} on the output of the \emph{Canny edge detector}. We had to backport this from OpenCV 3 as it is not available in the aforementioned version. As was to be expected judging from the examples in von Gioi's paper this detector requires significant post-processing. For every square in the grid the \emph{LSD} finds four line segments. Consequently two adjacent squares produce two parallel lines. This method is also prone to noisy backgrounds and produces many short, false positive lines. That is due to the nature of the detector of approximating circles with lines which comes into effect especially around black pieces, as can be seen in \autoref{fig:lsdPostprocessingFirst}.

	Therefore, our first postprocessing step is to eliminate short line segments (see \autoref{fig:lsdPostprocessingLength}). Afterwards we count the number of close-by parallels of each line and keep only those with a count of at least one. Having reduced the number of false positives significantly we classify the lines the same way as described above into horizontal and vertical lines. If we intersected these lines now, we would still have many false positives. That is, because one line which spans the whole length of the grid on the board might be represented by several detected line segments. As we prolong them into infinity before intersecting them, small directional variances sum up and result in intersections at the wrong place. Thus, as last postprocessing step, we merge nearby line segments into one if they are close and parallel to each other and filter once again for line length, this time with a slightly higher threshold.

	\begin{figure}
	\begin{center}
		\begin{subfigure}{0.33\textwidth}
			\includegraphics[width=\textwidth]{images/lsd_first.png}
			\caption{Unfiltered \emph{LSD} output}
			\label{fig:lsdPostprocessingFirst}
		\end{subfigure}
		\hspace{2em}
		\begin{subfigure}{0.33\textwidth}
			\includegraphics[width=\textwidth]{images/lsd_length.png}
			\caption{... filtered for line length,}
			\label{fig:lsdPostprocessingLength}
		\end{subfigure}
		\\
		\begin{subfigure}{0.33\textwidth}
			\includegraphics[width=\textwidth]{images/lsd_parallel.png}
			\caption{... filtered for at least one parallel, }
			\label{fig:lsdPostprocessingParallel}
		\end{subfigure}
		\hspace{2em}
		\begin{subfigure}{0.33\textwidth}
			\includegraphics[width=\textwidth]{images/lsd_final.png}
			\caption{and stitched and filtered once more for length}
			\label{fig:lsdPostprocessingFinal}
		\end{subfigure}

		\caption{The postprocessing steps necessary for the \emph{LSD}}
	\end{center}
	\end{figure}


	\subsection{Using corner detectors}
	\label{detector-visible-corners}
	Another option to detect the intersections directly without using line detection might be to use a corner detector. We tested the \emph{FAST} \cite{rosten2006machine} detector on a slightly blurred image. However it provides too many false positives to be useful for this task. Especially around black pieces it returns keypoints which do not belong to the grid. The \emph{ORB} detector which uses \emph{FAST} internally does not increase quality either.

	\section{Detecting occluded intersections}
	\label{detector-occluded}
	Intersection positions which could previously not be detected are typically occluded by tokens. As those have to be placed on intersections, we can in turn use their location to determine the underlying intersections. Both methods (\emph{Hough Circle Transformation} and a proprietary one based on structural analysis) which we tested need some preprocessing, though.

	\subsection{Preprocessing}
	\label{detector-occluded-preprocessing}
	To remove the lines and have only the pieces themselves remaining we tried to threshold the image. This works quite well for the black pieces. On the other hand the white pieces cannot be reliably detected this way. The reason for this is that the contrast between them and the board is too low. Therefore we turned to color images. We convert them from OpenCV's native BGR into HSV color space and threshold the value channel for black pieces. In this, we obtain good results.

	White pieces cannot be detected in the value channel because their contrast is too low. In the other channels they remain hard to detect, too, as the white balance of the camera input is constantly being adjusted by the operating system. This leads to color aberrations when a player puts down a token and his or her hand takes up a large portion of the image: the image will change its color temperature and white tokens become slightly blue. Often the aberration will be corrected after the hand leaves the image, but depending on the background this does not happen every time. We tried fixing the white balance upon starting to record, but it seems the camera needs to adjust its white balance from time to time. Otherwise the white pieces stand out even less against the background. We also tried to fix white balance settings by forcing camera parameters, but we could find none which worked under all tested illuminations.

	Luckily the white tokens are well detectable under normal circumstances in the saturation channel (saturation of the pieces is low and the saturation of the board is high, hue is undefined) and when the color is shifted in the hue channel (saturation of the pieces and the board is low, hue of the pieces is low).

	Therefore, we threshold the saturation and the hue channel separately, count the non-zero responses and use the individual results only if at most 20\% of the image is below the threshold.

	This yields pretty good results, however there is still a lot of noise in the thresholded image. Consequently we apply a Gaussian blur to the source image before thresholding and remove speckles afterwards by eroding and dilating the image slightly.

	After that white pieces are actually detected better than black ones, as we describe in \autoref{evaluation-occluded-optimization}. Anyhow it is not very important for us to have a perfect detection ratio in this step, because the results are not used to generate the end result but only to fill in gaps in the previously detected intersections.

	\subsection{Using Hough circles}
	\label{detector-occluded-hough}
	Go pieces respectively their representation in the thresholded imares are relatively round and can be detected using \emph{Hough circles} to some extent as long as the image is taken from a relatively high angle. If the camera is positioned near to the plane in which the board lies the circles become ellipses. In order to still match the tokens we have to choose the threshold, above which a circle center will be detected in the accumulator space, quite low. This results in many false positives if noise is located in some near-circle configuration by chance.

	The centers of the detected circles are then added to intersections and duplicates (determined by their distance) are removed.

	\subsection{Using contours}
	\label{detector-occluded-contours}
	Finding pieces with \emph{Hough Transformation} is slow, though, and we were not contempt with the result to speed ratio. That is why we investigated further and chose to try analyzing the image topologically \cite{suzuki1985topological} using OpenCV's \emph{findContours} function.

	We preprocess the image the same way as before. This results in blobs for each piece whose contours can be detected nicely. If those are approximately round they can then be fitted well into quadratic rectangles. If we encounter a rectangle which is about twice as long in one dimension as in the other, we split it in two. Then we discard all contours whose bounding box is not more or less quadratic. Lastly we filter the rectangles by their size to remove too small or too large squares which typically come from noise.

	The centers of the remaining squares are then added to intersections just like the centers of the circles when using \emph{Hough Transformation}.

	\section{Extrapolating the board}
	\label{detector-calculate}
	Having collected a set of probable intersections we now need to fill in gaps where neither lines nor pieces have been detected. Also, we need to classify if a detected intersection is valid and a part of the board. That is, we have to filter those which simply are intersections of prolonged grid lines with the outer edge of the board and other noise.

	First we calculate the average angle of the horizontal lines from the previous step and rotate the set of intersections to justify them. This does not fix perspective distortion but it allows us to sort the intersections by rows.

	All intersections within a vertical window of 15 pixels of each other are considered to belong to one row. They are then sorted by row first and by column in second order.

	Then we select a number of intersections in the center of the board by selecting all intersections inside a square around the image center. Hereby, we assume the innermost intersection on the board to be the closest (detected or yet undetected) one to the center of the image. This assumption can hardly be made in setups where the image is taken by bystanders or from randomly located cameras. We argue, though, that when trying to record a game a user will be able to adjust the camera location accordingly. Another possibility could be to prompt the user for the center of the board, which can be easily done via the touchscreen of the smart phone. We did not implement this approach, but it should yield equal quality.

	If the number of selected intersections is too small (we found 20 to be a well working count), we increase the window size and try again.

	Those intersections (shown in yellow in \autoref{fig:buildModel-input}) are now modeled as a subset of actual board intersections. We determine the median distance between neighboring intersections (using the average worked less well). Then we iterate row-wise over the selected intersections from the top left to the bottom right. For every intersection we save the current row and column (on the board, not the image) and add a key point to our model at this location. If we encounter a gap between two intersections that is larger than the median distance we increase the column count accordingly. The same happens in y-direction with the row count. If we find an intersection is an outlier to the left we increase the column count of all previous key points accordingly.

	\begin{figure}
		\begin{subfigure}{0.45\textwidth}
			\includegraphics[width=\textwidth]{images/buildingModel.png}
			\caption{Red: All detected intersections; yellow: selected intersections; orange: virtual intersections; green: the board center}
			\label{fig:buildModel-input}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.45\textwidth}
			\includegraphics[width=\textwidth]{images/builtModel.png}
			\caption{The resulting model. Notice that undetected intersections are left out creating homography. Green: the image center}
			\label{fig:buildModel-output}
		\end{subfigure}
		\caption{Building the model}
		\label{fig:buildModel}
	\end{figure}

	The interim result of this iteration is a model of a part of the board with equidistant key points where every key point correspondents to a selected intersection as shown in \autoref{fig:buildModel-output}.

	During the iteration we also add virtual intersections to fill in gaps in our intersection set (see the orange circles in \autoref{fig:buildModel-input}). This is necessary in case the board's center intersection has not been detected. From this filled up intersection set we select the intersection closest to the image center and assume it is the center of the board. Using this intersection's row and column count we can shift our model to the correct position within the board.

	Our updated interim result is now a model of a part of the board located correctly inside the complete model of the board. We can use this as input for the RANSAC algorithm \cite{fischler1981random}, as we made sure every key point corresponds to an intersection. Thus we can estimate a homography between the actual image and the hypothetical image of our modeled board.

	This serves three purposes. First, RANSAC tolerates outliers, i.e., intersections which have been detected slightly displaced. Second, we can use the transformation information to rectify subsequent images and increase detection quality (see \autoref{evaluation-prepostprocessing-perspectiveRectifying}). Lastly, the transformation matrix can also be used to warp a complete model of the board and lay it onto the image. After doing so we know the location of every intersection in our image -- including those, which have not been detected previously.

	\section{Refining intersection locations}
	\label{detector-postprocessing}
	After the interpolation step the mean error is still quite high, because small offsets in the selected intersections result in large errors when interpolating from them. To fix these, we first shift interpolated intersections to a closeby measured intersection, if one is available. Then we perform another interpolation step with all those intersections which have been moved, i.e., in which we can be confident. This step can be repeated several times until a desired detection quality has been reached or it yields no further improvement. This is also key to detecting larger boards, where this step can be used to first find a nine-by-nine subgrid and then interpolate from it to larger boards. We did not implement this, though, due to the lack of a 19x19 board.

	\iftoggle{lookNice}{\vfill \pagebreak}{}

	\section{Classifying intersections}
	\label{detector-classifying}
	\begingroup
	\setlength{\columnsep}{20pt}%
	\begin{wrapfigure}[15]{r}{0.45\textwidth}
		\centering
  		\vspace{-30pt}
	    \includegraphics[width=0.42\textwidth]{images/colorDetection.png}
	  	\caption{Detection windows on the thresholded, segmented image after highlight removal}
	\end{wrapfigure}
	Having located the intersections we can evaluate the image at their positions to find whether there is a piece or not and (if applicable) its color. First, we use an adaptive threshold. To remove noise within the rectangles we perform a connected-component analysis similarly to the preprocessing step. The result is a binary image showing only the lines and black pieces. Highlights within the black pieces from reflections are still a problem but can be removed surprisingly well by erosion and dilation.

	\endgroup

	Since we have rotated the intersections in \autoref{detector-calculate} our resulting model is rotated by the same amount. We undo this rotation now to realign the model with the actual input image. Finally we calculate the average pixel value in a window around each intersection and compare it to two thresholds, $T_w$ and $T_b$, such that \begin{equation}
		\frac{\sum^{I}_{i=0}\sum^{J}_{j=0}w(i,j)}{I*J} =
		\begin{cases}
		> T_{w} + C & \Rightarrow  \text{white}\\
		< T_{b} & \Rightarrow \text{black}\\
		\text{otherwise} & \Rightarrow \text{empty}
		\end{cases}
	\end{equation}
	where $C$ is a constant value that is added to the white threshold on the edge of the grid. The reason behind this is that the lines are not continued there and less black pixels are contributed by them.


	\section{Postprocessing}
	As we not only return the detected board but also intersections and piece locations, we have to realign them once more with the image. This is necessary because we have not just rotated them but also cropped the image in \autoref{detector-preprocessing}. To undo this, we shift the intersections by the negative offset of the cropped image inside the input image.

	The results still have to be smoothed over time. We decided to do this outside of the detection step, though. This has the advantage that the detector is agnostic of how its used -- to simply analyze a single image or as a part of a larger recognition software. For details about this accumulation step see \autoref{android-detector-usingResults}.

	We try to return only results which are sane. Thus we discard any result where one or more intersection lies outside of the input image or two intersections are too close to each other. In the first case it is obviously impossible to classify the intersection. In the second no accurate value could be determined, because we use windows for the classification step they would overlap if piece locations are too close. Therefore we do not use these results in any way.
