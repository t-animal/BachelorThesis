%!TEX root = ../Thesis.tex

\chapter{Creating the Android app}
\label{android}
	So far we have only described our detection algorithm, but not how to gather the data or how to interact with the user. Our goal was to create a simple user experience showcasing the algorithm rather than building a full featured app. Still we wanted to hide the technical details as far as useful to stay reasonably close to a usable endproduct.

	\section{Tethering in the framework}
	\label{android-framework}
	\subsection{Adding the source dependencies}
	\label{android-framework-dependencies}
	OpenCV for Android is available since September 2011 so we will not describe in detail how we set it up. In short the process is as following. The Android bindings for OpenCV can be downloaded from the official site and be compiled into a library which can then be included into one's project's \textit{lib}-folder. Alternatively the framework can be set as a dependency in Eclipse Android Development Tool and let it handle the compilation and dependency resolution, which is how we included the library.

	One could build all the OpenCV native code for the framework oneself. This implies building it for every possible target architecture (ARMv6, ARMv7, x86...) and shipping it with the app. This creates large binaries, at the benefit of having a standalone application. The recommended way on the other hand is to depend on the \textit{OpenCV Manager} app and ask the user on first start to install it, if necesarry. This manager will then download the correct OpenCV native binaries in the required version and share it between all applications needing it. We chose to do the latter, at least for our prototype.

	\subsection{Gathering data}
	\label{android-framework-gathering}
	On Android apps are structured into \textit{Activities}. Those roughly correspond to the screens visible to a user: If an app that has an overview screen, an editor screen and a preview screen, it will likely have three activities.

	Each activity is built out of \textit{Views} describing and implementing the user interface and this is where the OpenCV framework comes in: It offers a special \texttt{JavaCameraView}, which offers a callback to \texttt{CvCameraListener2} interfaces for delivery of CameraFrames and displays the frames on the screen. However it does not expose access to the camera and locks the device for itself. To have more control over the camera (for example we experimented with locking the exposure and white balance to fix the issue of color aberation under certain circumstances as described in \ref{detector-occluded-hough}) we subclassed it into \texttt{CameraManipulatingView}. Our final setup comprises a \texttt{DetectorActivity} which implements the \texttt{CvCameraListener2} interface and uses a \texttt{CameraManipulatingView}.

	After startup we call the \textit{OpenCV Manager} to load an instance of OpenCV 2.4.3. Once that's done our native code library is loaded and we start receiving camera information from the \texttt{CameraManipulatingView} while still having access to camera parameters via it. We can retrieve images out of the camera information -- either in grayscale or rgba colorspace.

	Each image is then compared to the last one. They are subtracted and if they differ significantly in more than 20 pixels, the resulting image is directly returned, thereby skipping the detector and improving FPS and energy consumption while the user moves the camera. It also pre-filters some frames in which too much movement occurs, for example because one player is putting down a piece.

	\section{Using the detector}
	\label{android-detector}
	When an image passes the movement filter it is being analyzed, for which there are two possible approaches. Either using Android OpenCV or the Java Native Interface (JNI) to call compiled C++ code. As the first is mainly a Java wrapper around C++ code, using it also involves costly JNI calls. Therefore it is more performant to implement the detector in C/C++ and have just one call into the detector. This is also how we chose to structure our application -- it is an easy way to increase performance. It means, however, that our application has to be compiled and shipped for every target architecture seperately using the Android NDK.

	To really benefit from the performance increase we decided to use our detector as a black box and have only one call into it. The method is called \texttt{detect} and takes multiple parameters.

	While we could allocate memory in the native part of our application and release it when the app is stopped, it is much easier to request the memory in the java part and let the garbage collector clean up the unneeded memory. Even for OpenCV matrices which have to be manually released this holds true, as another JNI call would be necesarry in the \texttt{onStop} callback of the Activity. Therefore all input/output-parameters are allocated or at least declared in the java class and filled or instanciated in the native part.

	The first and most obvious paramater to pass is the image to analyze. It is important to note that while OpenCV usually assumes that color images are in BGR(A)-format the Android bindings retrieve an RGBA-image from the camera. Thus, the image's colorspace has to be converted before passing it into the detector. As we encountered problems in numerous places when dealing with images whose color channels use 8-bit integers to encode their value, we decided to play it safe and switch to floating point channels at this stage, too. There might be concerns that due to differences in floating point precision some calculations might yield different results. We briefly looked into this and found that we either did no operation that needed this level of accuracy or it did not return different results.

	The detection result is being written into a \texttt{char} array with an entry for every intersection of the board. It contains a \texttt{0}, \texttt{b} or \texttt{w} character after detection, corresponding to an empty intersection, one with a black or with a white piece on it, respectively.

	In order to perform the postprocessing steps as noted in \ref{detector-postprocessing} we need to pass the previously detected intersection back into the detector, which is done via a \texttt{MatOfPoint2f}. This is a special kind of OpenCV matrix, consisting of only one column and $n$ rows. It is used to quickly pass data between Java and native code. While Java lists would have to be copied matrices use the same memory in both environments, which allows seamless data exchange.

	All the other parameters are there to output information from the detector. Among them are lists of detected intersections, pieces and intersections -- all of which are there rather for debugging purposes than to give useful information to the user.

	\section{User interaction}
	\label{android-ui}
	This information has to be displayed to the user. While often computer vision applications draw directly onto the analyzed image we decided to show the detected information seperately and debugging output on the image.

	Android offers methods to draw costum views via \texttt{Canvas} and \texttt{Paint} objects similarly to OpenCV. In order to have a comparable experience when using the detector on a desktop computer and via the app we decided, however, to draw the result by cropping the input imge and draw the result next to it using OpenCV.
